{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŒ The Hitchiker's Guide to the Galaxy of Reinforcement Learning ðŸŒŒ\n",
    "\n",
    "\n",
    "### What is Reinforcement Learning ?\n",
    "\n",
    "Reinforcement Learning is an area of machine learning that helps someone or _something_ to make decisions that lead to maximizing a given reward. The best way to picture this would be to think of scenario where you are teaching your dog new tricks. You give the dog a treat on succefully following your command, from the dog's perspective it's goal is to recieve the maximum number of treats. So it eventually learns to repeat the behavior that led it to get the most treats. This is the crux of Reinforcement Learning, an **_agent_**  (the dog) learning behaviors that lead to maximizing the rewards it recieves from the **_environment_** (you)\n",
    "\n",
    "\n",
    "This notebook is intended to be a guide for beginners in Reinforcement Learning (RL). It introduces the basic ideas of RL, how deep learning fits into the world of RL, and most importantly it teaches how to train a deep neural network to balance a stick on a cart ðŸ˜› . Here's what you will be seeing : \n",
    "\n",
    "### 1. Background\n",
    "        1. Markov Decision Processes\n",
    "        2. Returns and Discounted Returns\n",
    "        3. The policy function\n",
    "        4. Value functions and Action-Value functions\n",
    "\n",
    "### 2. Deep Q Learning\n",
    "        1. Deep Q learning\n",
    "        2. The Policy Net\n",
    "        3. The Target Net\n",
    "        4. Experience Replay Buffers\n",
    "        5. Exploration vs Exploitation\n",
    "        6. The Agent\n",
    "        7. The DQN training algorithm\n",
    "\n",
    "### 3. Practical DQNs\n",
    "        1. The Cartpole environment\n",
    "        2. Weights & Biases and Why it's Awesome\n",
    "        3. Training the DQNs\n",
    "        \n",
    "### 4. Extending DQNs beyond CartPole\n",
    "        1. The Breakout environment\n",
    "        2. Training on the breakout environment"
   ]
  },
  {
   "attachments": {
    "mdp.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "\n",
    "## Markov Decision Processes\n",
    "\n",
    "![mdp.png](attachment:mdp.png)\n",
    "\n",
    "Markov Decision Processes are a framework to mathematically define how our agent interacts with it's environment.\n",
    "Here, we assume that our _agent_ interacts with our _environment_ in a sequential manner over time. At every timestep ($t$), the _agent_ recieves a representation of the _environment's_ state ($S_t$). The agent then proceeds to interact with the _environment_ through an _action_ ($A_t$). The _environment_ then proceeds to transition to it's next state ($S_{t+1}$) granting the _agent_ a reward ($R_{t+1}$). This process repeats it self for the next time step ($t+1$).\n",
    "\n",
    "We can think of the reward giving process as an arbitrary function ($f$) that maps a state ($S_t$), and, an action ($A_t$) to a reward ($R_{t+1}$).\n",
    "\n",
    "$$f(S_t, A_t) = R_{t+1}$$\n",
    "\n",
    "Let $S$ be the set of all possible states the environment can be in, and, $A$ be the set of all possible actions the agent can take. Assuming both $S$ and $R$ are finite, we can think of $R_t$ and $S_t$ to be random variables, with well defined probabilities that depend on the previous _state-action pair_ ($S_{t-1}, R_{t-1}$). \n",
    "\n",
    "## Expected Returns\n",
    "\n",
    "The goal of our agent is to maximize the cumulative rewards, also called as the expected return. You can picture it as the sum of all future rewards it recieves from any given time step ($t$) till the final time step of the process ($T$). Let's denote this with $G_t$\n",
    "$$G_t = R_t + R_{t+1} +  R_{t+2} ... R_{T}$$\n",
    "$$G_t = \\sum_{k=t}^{T} R_k$$\n",
    "\n",
    "When a process has a final time step $T$, it naturally can be broken down into finite subsequences called _episodes_. For example, a game of pong has multiple rounds. Each round can be thought of as an _episode_ with the final time step happening when one player loses the round. However, not all tasks have final timesteps. For example, a factory robot painting cars. These kind of tasks are called continuing tasks, make our definition of expected return obselete as it grows to become $\\infty$ because $T=\\infty$.\n",
    "\n",
    "We can however create a new definition of expected return by _discounting_ the future rewards. We can progressively weigh future lower and lower so our expected return stays finite. We call this weighing term _discount rate_ denoted by ($\\gamma$). The expected return is now called _discounted return_ and our agent's goal is to maximize this.\n",
    "\n",
    "$$G_t = R_t + \\gamma R_{t+1} +  \\gamma^{2} R_{t+2} ... $$\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "$$G_t = R_t + \\gamma G_{t+1}$$\n",
    "\n",
    "The key takeway from the idea of the discounted return is that our agent now cares more about the immediate rewards than it does about the future rewards. This term helps us choose the trade off between short term rewards and long term rewards.\n",
    "\n",
    "## The Policy Function\n",
    "\n",
    "A policy function maps the probability of our agent picking a certain action ($a$), given it has observed a state ($s$) to a probabilty distribution. We can donate this by $\\pi$. Following a policy $\\pi$ means picking the action that yields the maximum probability. The optimal policy function always yeilds the highest probabilty to the action that yields the maximum discounted return. We can denote the optimal policy using $\\pi^{*}$ \n",
    "\n",
    "## Value functions and Value-Action Functions\n",
    "\n",
    "Value and Value-Action functions are functions that map the expected discounted return of a state or a state-action pair. What this means intuitively is that a value / value-action can tell the agent how good a specific state or state action pair is from the agent's perspective were it to follow a policy $\\pi$.\n",
    "\n",
    "Mathematically, the value function of a state ($s$) under a policy $\\pi$ can be defined as follows\n",
    "$$V_{\\pi}(s) = E[G_t | s]$$\n",
    "$$V_{\\pi}(s) = E[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ | S_t = s]$$\n",
    "\n",
    "Similarly, Value-Action function, often called as the **Q-value** function can be defined as follows\n",
    "$$Q_{\\pi}(s, a) = E[G_t | s]$$\n",
    "$$Q_{\\pi}(s, a) = E[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}| S_t = s, A_t = a]$$\n",
    "\n",
    "The goal of reinforcement learning is to estimate these value functions, inorder to determine the best actions the agent can take.\n",
    "\n",
    "## Deep Q Learning\n",
    "\n",
    "Recall that the optimal policy ($\\pi^{*}$) always assigns the highest probability to the action that yields the best expected discounted returns. So, any other policy ($\\pi$) can said to be optimal if and only if it yeilds better discounted returns for all states.\n",
    "\n",
    "$$\\pi \\geq \\pi^{*} \\text{      if and only if     } v_{\\pi}(s) \\geq v_{\\pi^{*}}(s) \\forall s \\epsilon S$$\n",
    "\n",
    "We know that the goal of Reinforcement Learning is to estimate the value functions. What is best way to approximate a function ? Neural Nets !. This is where Deep Learning fits in the realm of Reinforcement Learning.\n",
    "\n",
    "Only the optimal q function similarly returns the largest q value for all state action pairs.Therefore, it follows something called the **Bellman Optimality Equation**\n",
    "\n",
    "$$ Q_{*}(s, a) = E[ R_{t+1} + \\gamma \\max_{a'}Q_{*}(s', a') ] $$\n",
    "\n",
    "What this means is that the optimal q value for any state-action pair is the expectation of the reward recieved from the said state-action pair plus the optimal Q value achieved from any possible next state-action pairs.\n",
    "\n",
    "For any Q function $Q(s, a)$ to become the optimal Q function $Q_{*}(s, a)$, it has to follow the Bellman equation as well. Our neural network might not do so at the beginning, so all we have to do is minimize the loss between the Q-value it should be returning were it to follow the bellman optimality equation vs the value it actually returns.   \n",
    "$$loss = MSE|Q(s, a) -  (R_{t+1} + \\gamma \\max_{a'}Q(s', a'))|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleEnvManager():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.env    = gym.make('CartPole-v0').unwrapped\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        self.done           = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "        \n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space.n\n",
    "        \n",
    "    def take_action(self, action):        \n",
    "        _, reward, self.done, _ = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device)\n",
    "    \n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen        = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "    \n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "       \n",
    "    def get_processed_screen(self):\n",
    "        screen = self.render('rgb_array').transpose((2, 0, 1)) # PyTorch expects CHW\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[1]\n",
    "        \n",
    "        # Strip off top and bottom\n",
    "        top    = int(screen_height * 0.4)\n",
    "        bottom = int(screen_height * 0.8)\n",
    "        screen = screen[:, top:bottom, :]\n",
    "        return screen\n",
    "    \n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((40,90)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        \n",
    "        rate = strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).to(self.device) # exploit\n",
    "    \n",
    "    def test_agent(self, em, policy_net):\n",
    "        em.reset()\n",
    "        state = em.get_state()\n",
    "        frames = [em.render('rgb_array')]\n",
    "        total_reward = 0\n",
    "\n",
    "        for timestep in count():\n",
    "            action = policy_net(state).argmax(dim=1)\n",
    "            reward = em.take_action(action)\n",
    "            state = em.get_state()\n",
    "            frames.append(em.render('rgb_array'))\n",
    "            total_reward += reward.item()\n",
    "            if em.done or timestep > 1500:\n",
    "                break\n",
    "\n",
    "        em.close()\n",
    "        \n",
    "        return np.array(frames).transpose(0, 3, 1, 2), total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleDQN(nn.Module):\n",
    "    def __init__(self, img_height, img_width):\n",
    "        super().__init__()\n",
    "         \n",
    "        self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=24)   \n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=2)            \n",
    "\n",
    "    def forward(self, t):\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward', 'terminal')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences, device):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    \n",
    "    if torch.is_tensor(batch.state[0]):\n",
    "        t1 = torch.cat(batch.state).to(device)\n",
    "        t4 = torch.cat(batch.next_state).to(device)\n",
    "    else:\n",
    "        t1 = np.array([np.array(st).transpose(2, 0, 1) for st in batch.state])\n",
    "        t1 = torch.from_numpy(t1).to(device) / 255\n",
    "        t4 = np.array([np.array(st).transpose(2, 0, 1) for st in batch.next_state])\n",
    "        t4 = torch.from_numpy(t4).to(device) / 255\n",
    "    \n",
    "    \n",
    "    t2 = torch.cat(batch.action).to(device)\n",
    "    t3 = torch.cat(batch.reward).to(device)\n",
    "    t5 = torch.cat(batch.terminal).to(device)\n",
    "    \n",
    "    return (t1,t2,t3,t4,t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Experience(\\*zip(\\*experiences)) used above\n",
    "See https://stackoverflow.com/a/19343/3343043 for further explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Experience(state=1, action=1, next_state=1, reward=1, terminal=1),\n",
       " Experience(state=2, action=2, next_state=2, reward=2, terminal=1),\n",
       " Experience(state=3, action=3, next_state=3, reward=3, terminal=1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1 = Experience(1,1,1,1,1)\n",
    "e2 = Experience(2,2,2,2,1)\n",
    "e3 = Experience(3,3,3,3,1)\n",
    "\n",
    "experiences = [e1,e2,e3]\n",
    "experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=(1, 2, 3), action=(1, 2, 3), next_state=(1, 2, 3), reward=(1, 2, 3), terminal=(1, 1, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = Experience(*zip(*experiences))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Value Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states, terminal):\n",
    "        with torch.no_grad():\n",
    "            non_final_states  = next_states[~terminal] # get all non terminal states\n",
    "            batch_size        = next_states.shape[0]   # get the batch size\n",
    "            values            = torch.zeros(batch_size).to(device) # create a zero value tensor \n",
    "            values[~terminal] = target_net(non_final_states).max(dim=1)[0].detach() # replace zeros with Q values \n",
    "\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeacekurella\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.21<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">polished-monkey-193</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/peacekurella/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning\" target=\"_blank\">https://wandb.ai/peacekurella/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/peacekurella/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning/runs/3tm63i0s\" target=\"_blank\">https://wandb.ai/peacekurella/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning/runs/3tm63i0s</a><br/>\n",
       "                Run data is saved locally in <code>/home/prashanth/Desktop/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning/wandb/run-20210304_184302-3tm63i0s</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# login to weights and biases\n",
    "wandb.login()\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'DeepQLearning.ipynb' # set the name of the notebook\n",
    "\n",
    "# set up the run configuration\n",
    "config = dict(\n",
    "    batch_size         = 256,\n",
    "    eps_start          = 1,\n",
    "    eps_end            = 0.01,\n",
    "    eps_decay          = 0.001,\n",
    "    gamma              = 0.999,\n",
    "    learning_rate      = 0.001,\n",
    "    episodes           = 10000,\n",
    "    sync_time          = 10, \n",
    "    replay_buffer_size = 1000000,\n",
    "    loss_fn            = 'Huber',\n",
    "    warm_start         = 50000,\n",
    "    max_test_length    = 1500,\n",
    "    fps                = 24,\n",
    "    test_freq          = 10,\n",
    "    save_freq          = 100,\n",
    "    learn_start        = 0\n",
    ")\n",
    "\n",
    "# initialize wandb\n",
    "run             = wandb.init(project='Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning', config=config)\n",
    "artifact        = wandb.Artifact('policy_net', type='model')\n",
    "device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "run_videos      = os.path.join((str(run.name)))\n",
    "run_checkpoints = os.path.join(run_videos, 'ckpt')\n",
    "\n",
    "\n",
    "if not os.path.isdir(run_checkpoints):\n",
    "    os.makedirs(run_checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "em       = CartPoleEnvManager(device)\n",
    "tem      = CartPoleEnvManager(device)\n",
    "strategy = EpsilonGreedyStrategy(config['eps_start'], config['eps_end'], config['eps_decay'])\n",
    "agent    = Agent(strategy, em.num_actions_available(), device)\n",
    "memory   = ReplayMemory(config['replay_buffer_size'])\n",
    "\n",
    "policy_net = CartPoleDQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net = CartPoleDQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=config['learning_rate'])\n",
    "loss_fn   = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "for episode in range(config['episodes']):\n",
    "    \n",
    "    em.reset()\n",
    "    state          = em.get_state()\n",
    "    episode_reward = 0\n",
    "    episode_loss   = []\n",
    "    \n",
    "    for timestep in count():\n",
    "        \n",
    "        action = agent.select_action(state, policy_net)\n",
    "        reward = em.take_action(action)\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward, torch.tensor([em.done])))\n",
    "        state = next_state\n",
    "        episode_reward += reward.item()\n",
    "        \n",
    "        if memory.can_provide_sample(config['warm_start']) and timestep > config['learn_start']:\n",
    "            \n",
    "            experiences = memory.sample(config['batch_size'])\n",
    "            states, actions, rewards, next_states, terminal = extract_tensors(experiences, device)\n",
    "            \n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states, terminal)\n",
    "            target_q_values = (next_q_values * config['gamma']) + rewards\n",
    "            loss = loss_fn(current_q_values, target_q_values.unsqueeze(1).float())\n",
    "            episode_loss += [loss.detach().item()]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        if em.done:\n",
    "            run.log({\n",
    "                'Train/EpisodeReward' : episode_reward,\n",
    "                'Train/EpisodeDuration': timestep,\n",
    "                'Train/EpisodeLoss': sum(episode_loss) / len(episode_loss)\n",
    "            }, step = episode)\n",
    "            break\n",
    "\n",
    "    if episode % config['sync_time'] == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    # periodically test the model and save the episode videos\n",
    "    if episode % config['test_freq'] == 0:\n",
    "\n",
    "        # test the agent\n",
    "        frames, total_reward = agent.test_agent(tem, policy_net)\n",
    "\n",
    "        # expects the video to be of shape (t, c, h, w)\n",
    "        video = wandb.Video(\n",
    "            frames,\n",
    "            fps=config['fps'],\n",
    "            caption = str(episode),\n",
    "            format='mp4'\n",
    "        )\n",
    "        \n",
    "        # log the video and reward\n",
    "        run.log({\n",
    "            'Test/Video' : video,\n",
    "            'Test/EpisodeDuration': len(frames),\n",
    "            'Test/EpisodeReward': total_reward\n",
    "        }, step = episode)\n",
    "\n",
    "    # periodically save the model weights\n",
    "    if episode % config['save_freq'] == 0:\n",
    "\n",
    "        # save locally for back up\n",
    "        torch.save(policy_net.state_dict(), os.path.join(run_checkpoints, str(episode)))\n",
    "\n",
    "        # use wandb artifact to save the model\n",
    "        artifact.add_file(os.path.join(run_checkpoints, str(episode)))\n",
    "\n",
    "        \n",
    "# close the environment\n",
    "run.log_artifact(artifact)\n",
    "run.join()        \n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from stable_baselines.common.atari_wrappers import wrap_deepmind\n",
    "except:\n",
    "    from stable_baselines.common.atari_wrappers import wrap_deepmind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakoutEnvManager(CartPoleEnvManager):\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.env = gym.make('BreakoutDeterministic-v4')\n",
    "        self.env = wrap_deepmind(self.env, frame_stack = True)\n",
    "        self.reset()\n",
    "        self.done = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_state = self.env.reset()\n",
    "        \n",
    "    def take_action(self, action):        \n",
    "        self.current_state, reward, self.done, info = self.env.step(action.item())\n",
    "        if self.done: \n",
    "            reward = -1\n",
    "            if info['ale.lives'] > 0:\n",
    "                self.done = False\n",
    "        \n",
    "        return torch.tensor([reward], device=self.device)\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.current_state # lazy array (84, 84, 4), only convert to tensor before passing to model\n",
    "    \n",
    "    def get_screen_height(self):\n",
    "        return 84\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        return 84"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "wandbt",
   "language": "python",
   "name": "wandbt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

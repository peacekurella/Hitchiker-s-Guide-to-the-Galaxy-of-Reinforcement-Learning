{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "from collections import namedtuple \n",
    "from itertools import count\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtils():\n",
    "\n",
    "    @staticmethod\n",
    "    def save_episode(frames, episode, path):\n",
    "        \"\"\"\n",
    "        Saves the episode as a video, returns the path of the saved video\n",
    "\n",
    "        frames: A list of frames to be saved as the video\n",
    "        episode: Episode number \n",
    "        path: directory to save the frames in\n",
    "        \"\"\"\n",
    "\n",
    "        height,width,channels = frames[0].shape\n",
    "\n",
    "        path  = os.path.join(path, str(episode)+'.mp4')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video = cv2.VideoWriter(path, fourcc, 24, (width, height)) # Upscale for better quality\n",
    "\n",
    "        for frame in frames:\n",
    "            video.write(frame)\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        video.release()\n",
    "\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Named Tuple to hold our experiences \n",
    "\n",
    "Experience = namedtuple(\n",
    "    'Experience', # Class name\n",
    "    ( \n",
    "        'state', # start state\n",
    "        'action', # action taken\n",
    "        'reward', # reward recieved\n",
    "        'next_state', # next state,\n",
    "        'is_terminal' # store information about wether the state is terminal or not\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    Bounded buffer to hold experiences\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        \n",
    "        self.max_size = size # set the maximum size limit\n",
    "        self.push_count = 0 # set the push count to zero\n",
    "        self.buffer = [] # set the buffer to be an empty list\n",
    "        \n",
    "    def push(self, experience):\n",
    "        \n",
    "        # check if we have space in our buffer\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(experience) # add the experience to buffer if we have space \n",
    "        else:\n",
    "            self.buffer[self.push_count % self.max_size] = experience # deque oldest experience and add new one\n",
    "        \n",
    "        # increment push count\n",
    "        self.push_count += 1\n",
    "    \n",
    "    def is_samplable(self, batch_size):        \n",
    "        return len(self.buffer) >= batch_size\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        # return a random sample from the batch \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        batch = Experience(*zip(*batch))\n",
    "        \n",
    "        states = torch.cat(batch.state)\n",
    "        actions = torch.cat(batch.action)\n",
    "        rewards = torch.cat(batch.reward)\n",
    "        next_states = torch.cat(batch.next_state)\n",
    "        is_terminal = torch.cat(batch.is_terminal)\n",
    "        \n",
    "        return Experience(states, actions, rewards, next_states, is_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy(object):\n",
    "    \n",
    "    def __init__(self, start, end, decay):\n",
    "        \n",
    "        self.start = start # set the starting value of epsilon\n",
    "        self.end = end # set the ending value of epsilon\n",
    "        self.decay = decay # set the decay rate of epsilon\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        \n",
    "        # return an exponentially decaying epsilon\n",
    "        return self.end + (self.start - self.end) * math.exp(-1.0 * current_step * self.decay)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleEnvManager():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.env = gym.make('CartPole-v0').unwrapped\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "        \n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space.n\n",
    "        \n",
    "    def take_action(self, action):        \n",
    "        _, reward, self.done, _ = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device)\n",
    "    \n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "    \n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "       \n",
    "    def get_processed_screen(self):\n",
    "        screen = self.render('rgb_array').transpose((2, 0, 1)) # PyTorch expects CHW\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[1]\n",
    "        \n",
    "        # Strip off top and bottom\n",
    "        top = int(screen_height * 0.4)\n",
    "        bottom = int(screen_height * 0.8)\n",
    "        screen = screen[:, top:bottom, :]\n",
    "        return screen\n",
    "    \n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage()\n",
    "            ,T.Resize((40,90))\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        \n",
    "        rate = strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).to(self.device) # exploit \n",
    "    \n",
    "    def test_agent(self, policy_net, testEnv):\n",
    "\n",
    "        episode_frames = []\n",
    "        state  = testEnv.reset_environment()\n",
    "        test_reward = 0\n",
    "\n",
    "        # go through time steps\n",
    "        for t in range(config['max_test_length']):\n",
    "            action = policy_net(state.to(net_device)).argmax(dim=1) # get the action\n",
    "            state, reward, done, _, frame = testEnv.perform_action(action) # perform the action\n",
    "            test_reward += reward.item() # add the reward\n",
    "            episode_frames.append(frame.T)\n",
    "\n",
    "            # stop the loop when done\n",
    "            if done.item():\n",
    "                break\n",
    "    \n",
    "        return test_reward, episode_frames, len(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartpoleDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_height, img_width, num_actions):\n",
    "        super().__init__()\n",
    "         \n",
    "        self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=24)   \n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=num_actions)            \n",
    "\n",
    "    def forward(self, t):\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qvalues():\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        \"\"\"\n",
    "        Returns the Q(s, a) value of the current batch\n",
    "        \n",
    "        policy_net: policy network\n",
    "        states: batch of states of shape (B, C, H, W)\n",
    "        actions: batch of actions of shape(B, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # return the Q value of the corresponding actions\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_states, is_terminal):\n",
    "        \"\"\"\n",
    "        Returns the Q'(s', a') value of the current batch\n",
    "        \n",
    "        target_net: target network\n",
    "        next_states: batch of next states with shape (B, C, H, W)\n",
    "        is_terminal: boolean mask indication which states are terminal \n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values               = target_net(next_states).max(dim = 1)[0].detach() \n",
    "            q_values[is_terminal]  = 0\n",
    "            return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeacekurella\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.21<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">wild-morning-91</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/peacekurella/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning\" target=\"_blank\">https://wandb.ai/peacekurella/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/peacekurella/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning/runs/b9suq01f\" target=\"_blank\">https://wandb.ai/peacekurella/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning/runs/b9suq01f</a><br/>\n",
       "                Run data is saved locally in <code>/home/prashanth/Desktop/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning/wandb/run-20210303_195714-b9suq01f</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# login to weights and biases\n",
    "wandb.login()\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'Breakout-bot-v1.ipynb' # set the name of the notebook\n",
    "\n",
    "# set up the run configuration\n",
    "config = dict(\n",
    "    batch_size         = 256,\n",
    "    epsilon_start      = 1,\n",
    "    epsilon_end        = 0.01,\n",
    "    epsilon_decay      = 0.001,\n",
    "    gamma              = 0.999,\n",
    "    learning_rate      = 0.001,\n",
    "    episodes           = 1000,\n",
    "    sync_time          = 10, \n",
    "    replay_buffer_size = 100000,\n",
    "    loss_fn            = 'MSE',\n",
    "    min_buffer_size    = 256,\n",
    "    max_test_length    = 1500,\n",
    "    fps                = 4\n",
    ")\n",
    "\n",
    "# initialize wandb\n",
    "run = wandb.init(project='Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning', config=config)\n",
    "artifact = wandb.Artifact('policy_net', type='model')\n",
    "\n",
    "# choose the device to run the network on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set the directory to save videos of current run\n",
    "run_videos = os.path.join((str(run.name)))\n",
    "run_checkpoints = os.path.join(run_videos, 'ckpt')\n",
    "\n",
    "# make the checkpoint and video directories if they do not exist\n",
    "if not os.path.isdir(run_checkpoints):\n",
    "    os.makedirs(run_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-03de61723f9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# create the target and policy networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpolicy_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartpoleDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_screen_height\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_screen_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtarget_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartpoleDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_screen_height\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_screen_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-bcbbfec8a563>\u001b[0m in \u001b[0;36mget_screen_height\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_screen_height\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_processed_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-bcbbfec8a563>\u001b[0m in \u001b[0;36mget_processed_screen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# PyTorch expects CHW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_screen_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcrop_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-bcbbfec8a563>\u001b[0m in \u001b[0;36mtransform_screen_data\u001b[0;34m(self, screen)\u001b[0m\n\u001b[1;32m     72\u001b[0m         ])\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add a batch dimension (BCHW)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "#create the env managers\n",
    "em = CartPoleEnvManager(device) \n",
    "\n",
    "# create the epsilon scheduler\n",
    "strategy = EpsilonGreedyStrategy(config['epsilon_start'], config['epsilon_end'], config['epsilon_decay']) \n",
    "\n",
    "# create the agent\n",
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "\n",
    "# create the target and policy networks\n",
    "policy_net = CartpoleDQN(em.get_screen_height(), em.get_screen_width(), em.num_actions_available()).to(device)\n",
    "target_net = CartpoleDQN(em.get_screen_height(), em.get_screen_width(), em.num_actions_available()).to(device) \n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "wandb.watch(policy_net)\n",
    "\n",
    "# create the replay memory buffer\n",
    "replay_memory = ExperienceReplayBuffer(config['replay_buffer_size']) \n",
    "\n",
    "# create the optimizer\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=config['learning_rate']) \n",
    "\n",
    "# set the loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean') \n",
    "\n",
    "\n",
    "# keep going through the episode until it's done\n",
    "for episode in range(config['episodes']): # count is a py-function that keeps track of current iteration\n",
    "    \n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    episode_reward = 0\n",
    "    episode_loss   = 0\n",
    "\n",
    "    for timestep in count():\n",
    "        action     = agent.select_action(state, policy_net)\n",
    "        reward     = em.take_action(action)\n",
    "        next_state = em.get_state()\n",
    "        replay_memory.push(Experience(state, action, next_state, reward, em.done))\n",
    "        state      = next_state\n",
    "\n",
    "        if replay_memory.is_samplable(config['min_buffer_size']):\n",
    "\n",
    "            # sample the replay memory\n",
    "            states, actions, rewards, next_states, terminals = replay_memory.sample(config['batch_size'])\n",
    "            \n",
    "            # get the current Q values\n",
    "            current_q_values = Qvalues.get_current(\n",
    "                policy_net,\n",
    "                states.to(device),\n",
    "                actions.to(device)\n",
    "            )\n",
    "            \n",
    "            # get the next Q values\n",
    "            next_q_values = Qvalues.get_next(\n",
    "                target_net,\n",
    "                next_states.to(device),\n",
    "                terminals.to(device)\n",
    "            )\n",
    "            \n",
    "            # calculate target Q values\n",
    "            target_q_values = (next_q_values * config['gamma']) + rewards\n",
    "            print(current_q_values, target_q_values.shape)\n",
    "            \n",
    "            # calculate loss and backprop\n",
    "            loss = loss_fn(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        # if done,  log losses & statistics and start the next episode\n",
    "        if em.done:\n",
    "            run.log({\n",
    "                'EpisodeLoss':   episode_loss,\n",
    "                'Train/EpisodeDuration': timestep\n",
    "            })\n",
    "            break\n",
    "        \n",
    "    # sync the target and policy nets\n",
    "    if episode % config['sync_time'] == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "#     # periodically test the model and save the episode videos\n",
    "#     if episode % 10 == 0:\n",
    "\n",
    "#         # test the agent\n",
    "#         test_reward, episode_frames = agent.test_agent(policy_net, testEnv)\n",
    "\n",
    "#         # save the episode locally for back up\n",
    "#         DataUtils.save_episode(episode_frames, episode, run_videos)\n",
    "\n",
    "#         # expects the video to be of shape (t, c, h, w)\n",
    "#         video = wandb.Video(\n",
    "#             np.array(episode_frames).transpose(0, 3, 2, 1),\n",
    "#             fps=config['fps'],\n",
    "#             caption = str(episode),\n",
    "#             format='mp4'\n",
    "#         )\n",
    "        \n",
    "#         # log the video and reward\n",
    "#         wandb.log({\n",
    "#             'Test/Video' : video,\n",
    "#             'Test/EpisodeReward': len(episode_frames)\n",
    "#         }, step = episode)\n",
    "\n",
    "#     # periodically save the model weights\n",
    "#     if episode % 100 == 0:\n",
    "\n",
    "#         # save locally for back up\n",
    "#         torch.save(policy_net.state_dict(), os.path.join(run_checkpoints, str(episode)))\n",
    "\n",
    "#         # use wandb artifact to save the model\n",
    "#         artifact.add_file(os.path.join(run_checkpoints, str(episode)))\n",
    "\n",
    "        \n",
    "# close the environment\n",
    "run.log_artifact(artifact)\n",
    "run.join()\n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandbt",
   "language": "python",
   "name": "wandbt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

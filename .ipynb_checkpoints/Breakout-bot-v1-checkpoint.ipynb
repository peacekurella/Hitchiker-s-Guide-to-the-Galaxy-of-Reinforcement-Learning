{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "from collections import namedtuple \n",
    "from itertools import count\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentManager(object):\n",
    "    \"\"\"\n",
    "    Wrapper class to manage the Environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        \n",
    "        # set the environment\n",
    "        self.env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "        \n",
    "        # set the device\n",
    "        self.device = device\n",
    "        \n",
    "        # set the current state \n",
    "        self.current_state = []\n",
    "        \n",
    "        # set the number of actions\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        \n",
    "    def perform_action(self, action):\n",
    "        \"\"\"\n",
    "        Performs the action and returns the preprocessed\n",
    "        state\n",
    "        \n",
    "        action: action to perform, choose index from ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
    "        \"\"\"\n",
    "        \n",
    "        # pass the action to the environment\n",
    "        frame, reward, done, info = self.env.step(action.item())\n",
    "        \n",
    "        # preprocess the observation, reward\n",
    "        observation         = self.preprocess_state(frame)\n",
    "        reward_clipped      = self.preprocess_reward(reward)\n",
    "        done                = torch.tensor([done]).to(self.device)\n",
    "        \n",
    "        # update the current state\n",
    "        self.current_state = torch.cat([self.current_state[:, 1:], observation], axis=1)\n",
    "        \n",
    "        # return the current state, reward, done, info, reward in the same order\n",
    "        return self.current_state, reward_clipped, done, info, frame, reward \n",
    "        \n",
    "    def preprocess_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Preprocesses the reward\n",
    "        \n",
    "        reward: \n",
    "        \"\"\"\n",
    "        \n",
    "        # clip the rewards to prevent gradient explosion\n",
    "        reward = np.sign(reward)\n",
    "        return torch.tensor([reward]).to(self.device)\n",
    "        \n",
    "        \n",
    "    def preprocess_state(self, state):\n",
    "        \"\"\"\n",
    "        Preprocesses the state by downsampling it and \n",
    "        then converting it to a grayscale image\n",
    "        \n",
    "        state: \n",
    "        \"\"\"\n",
    "        \n",
    "        # set the target shape to be half that of original \n",
    "        target_shape = (int(state.shape[0] / 2), int(state.shape[1] / 2))\n",
    "        \n",
    "        # compose the transforms \n",
    "        transform = T.Compose([\n",
    "            T.ToTensor(), # convert state to a tensor\n",
    "            T.Grayscale(), # convert state to Grayscale \n",
    "            T.Resize(target_shape) # reshape the image to the target shape\n",
    "        ])\n",
    "        \n",
    "        # return the normalized image in (B, C, H, W) format\n",
    "        return transform(state).unsqueeze(0).to(self.device) / 255\n",
    "    \n",
    "    \n",
    "    def reset_environment(self):\n",
    "        \"\"\"\n",
    "        Resets the environment and returns the starting state\n",
    "        \"\"\"\n",
    "        \n",
    "        # reset the current env\n",
    "        obs = self.env.reset()\n",
    "        \n",
    "        # preprocess the state\n",
    "        obs = self.preprocess_state(obs)\n",
    "        \n",
    "        # set the current state to be the stack of starting positions\n",
    "        self.current_state = torch.cat([obs, obs, obs, obs], axis=1)\n",
    "        \n",
    "        # return the current state\n",
    "        return self.current_state\n",
    "    \n",
    "    def close_environment(self):\n",
    "        \"\"\"\n",
    "        Close the environment\n",
    "        \"\"\"\n",
    "        \n",
    "        # close the environment\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtils():\n",
    "\n",
    "    @staticmethod\n",
    "    def display_state(state):\n",
    "        \"\"\"\n",
    "        Displays the preprocessed state\n",
    "\n",
    "        state: numpy array of shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(state.shape[0], state.shape[1])\n",
    "        for i in range(state.shape[0]):\n",
    "            for j in range(state.shape[1]):\n",
    "                axes[(i*j) + j].imshow(state[0, i], cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def save_episode(frames, episode, path):\n",
    "        \"\"\"\n",
    "        Saves the episode as a video, returns the path of the saved video\n",
    "\n",
    "        frames: A list of frames to be saved as the video\n",
    "        episode: Episode number \n",
    "        path: directory to save the frames in\n",
    "        \"\"\"\n",
    "\n",
    "        height,width,channels = frames[0].shape\n",
    "\n",
    "        path  = os.path.join(path, str(episode)+'.mp4')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video = cv2.VideoWriter(path, fourcc, 24, (width, height)) # Upscale for better quality\n",
    "\n",
    "        for frame in frames:\n",
    "            video.write(frame)\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        video.release()\n",
    "\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Named Tuple to hold our experiences \n",
    "\n",
    "Experience = namedtuple(\n",
    "    'Experience', # Class name\n",
    "    ( \n",
    "        'state', # start state\n",
    "        'action', # action taken\n",
    "        'reward', # reward recieved\n",
    "        'next_state', # next state,\n",
    "        'is_terminal' # store information about wether the state is terminal or not\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    Bounded buffer to hold experiences\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        \n",
    "        self.max_size = size # set the maximum size limit\n",
    "        self.push_count = 0 # set the push count to zero\n",
    "        self.buffer = [] # set the buffer to be an empty list\n",
    "        \n",
    "    def add_experience(self, experience):\n",
    "        \"\"\"\n",
    "        Adds an experience to the buffer\n",
    "        \n",
    "        experience: An experience class namedtuple \n",
    "        \"\"\"\n",
    "        \n",
    "        # check if we have space in our buffer\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(experience) # add the experience to buffer if we have space \n",
    "        else:\n",
    "            self.buffer[self.push_count % self.max_size] = experience # deque oldest experience and add new one\n",
    "        \n",
    "        # increment push count\n",
    "        self.push_count += 1\n",
    "    \n",
    "    def is_samplable(self, batch_size):\n",
    "        \"\"\"\n",
    "        Check if we can sample experiences\n",
    "        \n",
    "        batch_size: The batch size of samples\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.buffer) >= batch_size\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Samples a random set of experiences\n",
    "        \n",
    "        batch_size: sample size\n",
    "        \"\"\"\n",
    "        \n",
    "        # return a random sample from the batch \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        batch = Experience(*zip(*batch))\n",
    "        \n",
    "        states = torch.cat(batch.state)\n",
    "        actions = torch.cat(batch.action)\n",
    "        rewards = torch.cat(batch.reward)\n",
    "        next_states = torch.cat(batch.next_state)\n",
    "        is_terminal = torch.cat(batch.is_terminal)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, is_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy(object):\n",
    "    \"\"\"\n",
    "    Defines the strategy for exploration vs exploitation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, start, end, decay):\n",
    "        \n",
    "        self.start = start # set the starting value of epsilon\n",
    "        self.end = end # set the ending value of epsilon\n",
    "        self.decay = decay # set the decay rate of epsilon\n",
    "        \n",
    "    def get_epsilon(self, current_step):\n",
    "        \"\"\"\n",
    "        Returns the epsilon value for the current step\n",
    "        \n",
    "        current_step: Current step number\n",
    "        \"\"\"\n",
    "        \n",
    "        # return an exponentially decaying epsilon\n",
    "        return self.end + (self.start - self.end) * math.exp(-1.0 * current_step * self.decay)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"\n",
    "    Defines the agent \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions, strategy, device):\n",
    "        \n",
    "        self.steps = 0 # set the number of steps taken\n",
    "        self.strategy = strategy # set the exploration strategy\n",
    "        self.num_actions = num_actions # set the number of actions possible\n",
    "        self.device = device # set the device \n",
    "        \n",
    "    def choose_action(self, policy_net, state):\n",
    "        \"\"\"\n",
    "        Chooses an action depending on the strategy\n",
    "        \n",
    "        policy_net: policy network\n",
    "        state: Agent's state in the env\n",
    "        \"\"\"\n",
    "        \n",
    "        # if random value is less than epsilon, then explore\n",
    "        if random.random() < self.strategy.get_epsilon(self.steps):\n",
    "            action = random.randrange(self.num_actions) # perform a random action to explore env\n",
    "            self.steps += 1\n",
    "            return torch.tensor([action]).to(self.device)\n",
    "            \n",
    "        # return the action with the max Q value\n",
    "        self.steps += 1\n",
    "        return policy_net(state).argmax(dim = 1).to(self.device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines our Q network \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions):\n",
    "        \n",
    "        super(Qnetwork, self).__init__() # call the super class constructor\n",
    "        self.num_actions = num_actions # set the number of possible actions\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size = 8, stride = 4) # 4 channels in, 16 channels out\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size = 4, stride = 2) # 16 channels in, 32 channels out\n",
    "        \n",
    "        self.fc1 = nn.Linear(32 * 11 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, num_actions)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        defines the forward pass through the network\n",
    "        \n",
    "        state: Input state to the network\n",
    "        \"\"\"\n",
    "        \n",
    "        out = self.conv1(state) # pass through first conv layer\n",
    "        out = F.relu(out) # apply the relu activation\n",
    "        \n",
    "        out = self.conv2(out) # pass through second conv layer\n",
    "        out = F.relu(out) # apply the relu activation\n",
    "        \n",
    "        out = out.view(-1, 32 * 11 * 8) # flatten the conv output\n",
    "        \n",
    "        out = self.fc1(out) # pass through first fc layer\n",
    "        out = F.relu(out) # apply the relu activation\n",
    "        \n",
    "        out = self.fc2(out) # pass through second fc layer\n",
    "        out = F.relu(out) # apply the relu activation\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qvalues():\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_q_values(policy_net, states, actions):\n",
    "        \"\"\"\n",
    "        Returns the Q(s, a) value of the current batch\n",
    "        \n",
    "        policy_net: policy network\n",
    "        states: batch of states of shape (B, C, H, W)\n",
    "        actions: batch of actions of shape(B, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # return the Q value of the corresponding actions\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_q_prime_values(target_net, next_states, is_terminal):\n",
    "        \"\"\"\n",
    "        Returns the Q'(s', a') value of the current batch\n",
    "        \n",
    "        target_net: target network\n",
    "        next_states: batch of next states with shape (B, C, H, W)\n",
    "        is_terminal: boolean mask indication which states are terminal \n",
    "        \"\"\"\n",
    "        \n",
    "        q_values = target_net(next_states) # get the Q values of the batch \n",
    "        q_values[is_terminal] = 0 # zero out the Q values of terminal states\n",
    "        return q_values.max(axis=1).values.reshape(-1, 1) # return the max Q value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeacekurella\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.21<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">major-durian-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/peacekurella/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning\" target=\"_blank\">https://wandb.ai/peacekurella/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/peacekurella/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning/runs/lrg8zd5o\" target=\"_blank\">https://wandb.ai/peacekurella/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning/runs/lrg8zd5o</a><br/>\n",
       "                Run data is saved locally in <code>/home/prashanth/Desktop/Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning/wandb/run-20210302_223540-lrg8zd5o</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# login to weights and biases\n",
    "wandb.login()\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'Breakout-bot-v1.ipynb' # set the name of the notebook\n",
    "\n",
    "# set up the run configuration\n",
    "config = dict(\n",
    "    batch_size         = 32,\n",
    "    epsilon_start      = 1,\n",
    "    epsilon_end        = 0.1,\n",
    "    epsilon_decay      = 1e-4,\n",
    "    gamma              = 0.99,\n",
    "    learning_rate      = 0.0001,\n",
    "    episodes           = 1000000,\n",
    "    sync_time          = 1000, \n",
    "    replay_buffer_size = 40000,\n",
    "    loss_fn            = 'Huber'\n",
    ")\n",
    "\n",
    "# initialize wandb\n",
    "run = wandb.init(project='Hitchiker-s-Guide-to-the-Galaxy-of-Reinforcement-Learning', config=config)\n",
    "artifact = wandb.Artifact('policy_net', type='model')\n",
    "\n",
    "net_device = 'cuda:0' # device to put the networks on\n",
    "mem_device = 'cpu:0' # device to put the agent, env, and replay buffer on\n",
    "\n",
    "# set the directory to save videos of current run\n",
    "run_videos = os.path.join((str(run.name)))\n",
    "run_checkpoints = os.path.join(run_videos, 'ckpt')\n",
    "\n",
    "# make the checkpoint and video directories if they do not exist\n",
    "if not os.path.isdir(run_checkpoints):\n",
    "    os.makedirs(run_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the env manager on cpu to save gpu memory\n",
    "envMan = EnvironmentManager(mem_device) \n",
    "\n",
    "# create the epsilon scheduler\n",
    "strategy = EpsilonGreedyStrategy(config['epsilon_start'], config['epsilon_end'], config['epsilon_decay']) \n",
    "\n",
    "# create the agent\n",
    "agent = Agent(envMan.num_actions, strategy, mem_device) \n",
    "\n",
    "policy_net = Qnetwork(envMan.num_actions).to(net_device) # create the policy net\n",
    "target_net = Qnetwork(envMan.num_actions).to(net_device) # create the target net\n",
    "target_net.load_state_dict(policy_net.state_dict())  # sync the weights of target and policy nets\n",
    "target_net.eval() # put the target network in eval mode to make sure we dont change it's weights accidentally\n",
    "\n",
    "# create the replay memory buffer\n",
    "replay_memory = ExperienceReplayBuffer(config['replay_buffer_size']) \n",
    "\n",
    "# create the optimizer\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=config['learning_rate']) \n",
    "\n",
    "# set the loss function as Huber loss\n",
    "loss_fn = torch.nn.SmoothL1Loss(reduction='mean') \n",
    "\n",
    "\n",
    "# Go through all the episodes\n",
    "for episode in range(config['episodes']):\n",
    "    \n",
    "    # reset the state at the start of every episode\n",
    "    state = envMan.reset_environment()\n",
    "    episode_reward_clipped = 0\n",
    "    episode_reward         = 0\n",
    "    episode_loss           = 0\n",
    "    episode_frames         = []\n",
    "    \n",
    "    # keep going through the episode until it's done\n",
    "    for timestep in count(): # count is a py-function that keeps track of current iteration\n",
    "\n",
    "        # render the env, comment next line to stop showing the network playing\n",
    "        envMan.env.render()\n",
    "\n",
    "        # make the agent choose an action\n",
    "        action = agent.choose_action(policy_net, state.to(net_device))\n",
    "\n",
    "        # perform the action\n",
    "        next_state, reward, done, info, frame, reward_unc = envMan.perform_action(action)\n",
    "        episode_reward_clipped += reward.item()\n",
    "        episode_reward         += reward_unc\n",
    "        episode_frames.append(frame.T)\n",
    "        \n",
    "        # add the experience to replay memory, then update the current state \n",
    "        replay_memory.add_experience(Experience(state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        \n",
    "        # check if replay memory is samplable\n",
    "        if replay_memory.is_samplable(config['batch_size']):\n",
    "            \n",
    "            # sample the replay memory\n",
    "            states, actions, rewards, next_states, is_terminal = replay_memory.sample(config['batch_size'])\n",
    "            \n",
    "            # get the values of Q and Q_prime\n",
    "            q_value = Qvalues.get_q_values(policy_net, states.to(net_device), actions.to(net_device))\n",
    "            q_prime = Qvalues.get_q_prime_values(target_net, next_states.to(net_device), is_terminal.to(net_device))\n",
    "            q_star  = (rewards.unsqueeze(-1).to(net_device) + (config['gamma'] * q_prime)).float() # refer to the bellman eqn\n",
    "        \n",
    "            # calculate loss, do back prop\n",
    "            loss = loss_fn(q_value, q_star)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            episode_loss += loss.item() # track the episode loss\n",
    "            \n",
    "        # if done, log losses & statistics and start the next episode\n",
    "        if done.item():\n",
    "            run.log({\n",
    "                'EpisodeLoss':   episode_loss,\n",
    "                'EpisodeReward': episode_reward_clipped,\n",
    "                'EpisodeRewardUnclipped': episode_reward,\n",
    "                'EpisodeLength': len(episode_frames)\n",
    "            })\n",
    "            break\n",
    "    \n",
    "    # sync the target and policy nets\n",
    "    if timestep % config['sync_time'] == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    # periodically save the episode videos\n",
    "    if episode % 1000 == 0:\n",
    "        \n",
    "        # save the episode locally for back up\n",
    "        DataUtils.save_episode(episode_frames, episode, run_videos)\n",
    "        \n",
    "        # expects the video to be of shape (t, c, h, w)\n",
    "        video = wandb.Video(np.array(episode_frames).swapaxes(2, 3), fps=24, caption = str(episode))\n",
    "        wandb.log({'Episode-' + str(episode) : video})\n",
    "    \n",
    "    # periodically save the model weights\n",
    "    if episode % 10000 == 0:\n",
    "        \n",
    "        # save locally for back up\n",
    "        torch.save(policy_net.state_dict(), os.path.join(run_checkpoints, str(episode)))\n",
    "        \n",
    "        # use wandb artifact to save the model\n",
    "        artifact.add_file(os.path.join(run_checkpoints, str(episode)))\n",
    "        \n",
    "        \n",
    "# close the environment\n",
    "run.log_artifact(artifact)\n",
    "run.join()\n",
    "envMan.close_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandbt",
   "language": "python",
   "name": "wandbt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
